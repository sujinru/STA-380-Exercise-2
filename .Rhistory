# Read in all texts
library(tm)
library(naivebayes)
train_author_dirs = Sys.glob('~/Downloads/ReutersC50/C50train/*')
file_list = NULL
labels = NULL
for(author in train_author_dirs) {
author_name = substring(author, first=29)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
labels = append(labels, rep(author_name, length(files_to_add)))
}
test_author_dirs = Sys.glob('~/Downloads/ReutersC50/C50test/*')
for(author in test_author_dirs) {
author_name = substring(author, first=29)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
labels = append(labels, rep(author_name, length(files_to_add)))
}
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en')}
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
my_corpus = Corpus(VectorSource(all_docs))
# Create labels for traing and test set
clean_labels = NULL
for (i in 1:5000){
clean_labels = append(clean_labels, strsplit(labels[i], '/')[[1]][3])
}
train_y = clean_labels[1:2500]
test_y = clean_labels[2501:5000]
# function for train and test, given the training matrix
test = function(X){
train_X = X[1:2500,]
test_X = X[2501:5000,]
model <- naive_bayes(x = train_X, y = train_y)
preds <- predict(model, newdata = test_X)
conf_matrix <- table(preds, test_y)
sum = 0
for (i in 1:50){
sum = sum +  conf_matrix[i, i]
}
return(sum/2500)
}
# Preprocessing: removing
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), union(stopwords("SMART"), stopwords('en')))
DTM = DocumentTermMatrix(my_corpus)
DTM = removeSparseTerms(DTM, 0.895)
X = as.matrix(DTM)
TF <- X / rowSums(X)
EXI_NUM<-apply(X>0, 2, function(x){table(x)['TRUE']})
IDF<-as.numeric(log(nrow(X)/EXI_NUM + 1))
TFIDF = data.frame(t(t(TF)*IDF))
test(as.matrix(TFIDF))
X = as.matrix(DTM)
TF <- X / rowSums(X)
EXI_NUM<-apply(X>0, 2, function(x){table(x)['TRUE']})
IDF<-as.numeric(log(nrow(X)/EXI_NUM + 1))
TFIDF = data.frame(t(t(TF)*IDF))
test(as.matrix(TFIDF))
pca = prcomp(X, scale=TRUE)
X = as.matrix(DTM)
pca = prcomp(X, scale=TRUE)
pca = prcomp(X)
pca = prcomp(X, scale=TRUE)
X
DTM = removeSparseTerms(DTM, 0.895)
X = as.matrix(DTM)
pca = prcomp(X, scale=TRUE)
# Read in all texts
library(tm)
library(naivebayes)
train_author_dirs = Sys.glob('~/Downloads/ReutersC50/C50train/*')
file_list = NULL
labels = NULL
for(author in train_author_dirs) {
author_name = substring(author, first=29)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
labels = append(labels, rep(author_name, length(files_to_add)))
}
test_author_dirs = Sys.glob('~/Downloads/ReutersC50/C50test/*')
for(author in test_author_dirs) {
author_name = substring(author, first=29)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
labels = append(labels, rep(author_name, length(files_to_add)))
}
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en')}
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
my_corpus = Corpus(VectorSource(all_docs))
# Create labels for traing and test set
clean_labels = NULL
for (i in 1:5000){
clean_labels = append(clean_labels, strsplit(labels[i], '/')[[1]][3])
}
train_y = clean_labels[1:2500]
test_y = clean_labels[2501:5000]
# function for train and test, given the training matrix
test = function(X){
train_X = X[1:2500,]
test_X = X[2501:5000,]
model <- naive_bayes(x = train_X, y = train_y)
preds <- predict(model, newdata = test_X)
conf_matrix <- table(preds, test_y)
sum = 0
for (i in 1:50){
sum = sum +  conf_matrix[i, i]
}
return(sum/2500)
}
# Preprocessing: removing
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), union(stopwords("SMART"), stopwords('en')))
DTM = DocumentTermMatrix(my_corpus)
acc = NULL
per_list = seq(0.875, 0.925, 0.01)
for (i in per_list){
DTM_test = removeSparseTerms(DTM, i)
X = as.matrix(DTM_test)
acc = append(acc, test(X))
}
plot(per_list, acc, type='b', ylab='Accuracy', xlab='Percentage Threshold')
DTM = removeSparseTerms(DTM, 0.895)
max(acc)
X = as.matrix(DTM)
pca = prcomp(X, scale=TRUE)
dim(X)
dim(DTM)
# Read in all texts
library(tm)
library(naivebayes)
train_author_dirs = Sys.glob('~/Downloads/ReutersC50/C50train/*')
file_list = NULL
labels = NULL
for(author in train_author_dirs) {
author_name = substring(author, first=29)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
labels = append(labels, rep(author_name, length(files_to_add)))
}
test_author_dirs = Sys.glob('~/Downloads/ReutersC50/C50test/*')
for(author in test_author_dirs) {
author_name = substring(author, first=29)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
labels = append(labels, rep(author_name, length(files_to_add)))
}
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en')}
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
my_corpus = Corpus(VectorSource(all_docs))
# Create labels for traing and test set
clean_labels = NULL
for (i in 1:5000){
clean_labels = append(clean_labels, strsplit(labels[i], '/')[[1]][3])
}
train_y = clean_labels[1:2500]
test_y = clean_labels[2501:5000]
# function for train and test, given the training matrix
test = function(X){
train_X = X[1:2500,]
test_X = X[2501:5000,]
model <- naive_bayes(x = train_X, y = train_y)
preds <- predict(model, newdata = test_X)
conf_matrix <- table(preds, test_y)
sum = 0
for (i in 1:50){
sum = sum +  conf_matrix[i, i]
}
return(sum/2500)
}
# Preprocessing: removing
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), union(stopwords("SMART"), stopwords('en')))
DTM = DocumentTermMatrix(my_corpus)
DTM = removeSparseTerms(DTM, 0.895)
X = as.matrix(DTM)
test(X)
max(acc)
X <- TFIDF
pca = prcomp(X, scale=TRUE)
plot(cumsum((pca$sdev)^2)/sum(pca$sdev^2), pch=19, cex=0.1, ylab='Cumulative Variance', xlab='Number of Components' )
pca_list = seq(101, 110, 1)
acc = NULL
for (i in pca_list){
X_test = pca$x[, 1:i]
acc = append(acc, test(X_test))
}
plot(pca_list, acc, type='b', ylab='Accuracy', xlab='Number of Components')
max(acc)
pca_list = seq(50, 150, 0)
acc = NULL
pca_list = seq(50, 150, 0)
pca_list = seq(50, 150, 10)
acc = NULL
for (i in pca_list){
X_test = pca$x[, 1:i]
acc = append(acc, test(X_test))
}
plot(pca_list, acc, type='b', ylab='Accuracy', xlab='Number of Components')
max(acc)
plot(pca_list, acc, type='b', ylab='Accuracy', xlab='Number of Components')
pca_list = seq(115, 125, 1)
acc = NULL
for (i in pca_list){
X_test = pca$x[, 1:i]
acc = append(acc, test(X_test))
}
plot(pca_list, acc, type='b', ylab='Accuracy', xlab='Number of Components')
max(acc)
plot(pca_list, acc, type='b', ylab='Accuracy', xlab='Number of Components')
max(acc)
install.packages("nycflights13")
# install useful library
library(dplyr)
library(nycflights13)
library(maps)
library(ggplot2)
library(ggthemes)
install.packages("ggthemes")
# install useful library
library(dplyr)
library(nycflights13)
library(maps)
library(ggplot2)
library(ggthemes)
library(RColorBrewer)
library(Image)
library(RColorBrewer)
ggplot(data = states) +
geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = "white") +
coord_fixed(1.3) + scale_fill_manual(values = getPalette(colourCount)) +
guides(fill=FALSE) + geom_point(aes(x = cancelled_location$lon, y = cancelled_location$lat, size=cancelled_location$average), data = cancelled_location, color='blue4') + geom_text(aes(x = cancelled_location$lon, y = cancelled_location$lat-0.7, label = cancelled_location$Dest, size=0.7), data = cancelled_location) + theme_fivethirtyeight() + ggtitle('Average Cancellation Rate for Each Airport')
states = map_data('state')
ggplot(data = states) +
geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = "white") +
coord_fixed(1.3) + scale_fill_manual(values = getPalette(colourCount)) +
guides(fill=FALSE) + geom_point(aes(x = cancelled_location$lon, y = cancelled_location$lat, size=cancelled_location$average), data = cancelled_location, color='blue4') + geom_text(aes(x = cancelled_location$lon, y = cancelled_location$lat-0.7, label = cancelled_location$Dest, size=0.7), data = cancelled_location) + theme_fivethirtyeight() + ggtitle('Average Cancellation Rate for Each Airport')
cancelled_location = merge(merged_delete, usairports, by.x="Dest", by.y = "faa", all.x = TRUE)
# install useful library
options(warn=-1)
library(dplyr)
library(nycflights13)
library(maps)
library(ggplot2)
library(ggthemes)
library(RColorBrewer)
